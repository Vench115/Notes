[[2502.03544] Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2](https://arxiv.org/abs/2502.03544)

[AlphaGeometry2: Deepmind AI outperforms math Olympians at geometry tasks](https://the-decoder.com/alphageometry2-deepmind-ai-outperforms-math-olympians-at-geometry-tasks/)

# Abstract

作者团队推出了Alpha Geometry 2，一个由[[Alpha Geometry]]提出的Alpha Geometry的极大改进版本，目前，该版本已经在求解奥林匹克级别的几何问题中，超越了平均金牌选手的水平。为了达到该目标，作者首先扩展了原本的AG语言，来求解(tackle)更难的问题，包括对象的运动，以及包括角度、比例、和距离的线性方程组问题。这一步，以及其他内容的增加，极大地改进了AG语言在IMO 2000-2024几何问题上的覆盖率，从66%提升到了88%。

AG2的搜索过程也有了很大的改进，通过使用Gemini的架构，来达到更好的语言建模(language modeling)，以及一个全新的知识共享(knowledge-sharing)机制，集合了多重搜索树。

除了进一步加强符号引擎和综合数据生成，作者也大力地加速了AG2对过去25年里所有几何问题的整体求解率，从先前的54%提升到了84%。

AG2也是在IMO 2024中达到银牌等级的求解系统的一部分。最后，同样重要的是，作者团队报告了整个过程，针对使用AG2作为一个完全自动化系统的一部分，来直接从自然语言输入中可靠地(reliably)求解几何问题。

# Introduction

## Background

IMO对于全世界的高中学生来说，都是一个享誉盛名的数学竞赛。IMO问题以它们的高难度而被人所熟知，而且求解这些问题需要对于数学概念的深度理解，以及创造性地应用它们的能力。几何，四大IMO问题范畴之一，是几类问题中形式最统一的一个，因此也最容易理解(approachable)。这类问题也非常适合基本的推理研究。

## Approaches

自动化求解几何问题，主要有两种技术路径。一种，是用代数方法“痛击”(bashing)问题，利用比如“吴法”；面积法；或者Grobner基方法。另一种，则依赖于合成技巧(synthetic techniques)例如推理数据库、或全角法。作者关注于后者，作为一种更加与人类似的方法路径，也适合于将研究知识转移到其他领域。在作者先前的工作中，作者提出了AlphaGeometry(AG1)，一个神经-符号系统，展现了完全统治(mastering)该领域的一大步，达到了在2000-2024全部IMO几何问题上54%的求解率。AG1结合了一个语言模型和一个符号引擎，来有效地解决这些极具挑战性的问题。

##  Limitations

尽管它取得了成功，AG1在几个关键领域还是展现出了局限性。它的性能被它领域专用语言的范围、符号引擎的效率、以及初始的语言模型的能力所限制。因此，当考虑到最近从2000年至今的所有IMO几何问题，AG1仅能达到54%的求解率。

## Improvements

该文章则提出了AlphaGeometry2(AG2)，是对AG1的大幅升级，突破了这些限制，而且增强了性能。AG2利用(leverages)一个更强大的，基于Gemini的语言模型，它是在一个更大更多样的数据集上进行训练的。

作者也提出了一个显著更快，且更加鲁棒的符号引擎，也加入了很多优化，例如：一个简化的规则集合，以及更强的对双点的处理能力。

不仅如此，作者团队扩展了领域语言，以覆盖范围更广的几何概念，包括轨迹定理(locus theorems)以及线性方程。

为了进一步改善性能，作者开发了一个全新的搜索算法，能够探索更宽范围的辅助构造策略，并运用一个知识共享的机制，来扩展并加速搜索过程。

最终，作者在构建一个完全自动化的而且可靠的系统上取得了进展，该系统能够以自然语言的方式求解几何问题。为了实现该目标，作者使用Gemini将问题从自然语言翻译成AG的语言，并且实现了一个新的自动化图生成算法。

## AG2 Key Improvements

- Expanded Domain Language(扩展的领域专用语言)：覆盖轨迹型的定理、线性方程、以及非构造性的(non-constructive)问题声明。
- Stronger and faster Symbolic Engine(更强更快的符号引擎)：优化的规则集合，新增的对于双点的处理，以及更快的用C++的实现。
- Advanced Novel Search Algorithm：利用多重的搜索树，且利用知识共享。
- Enhanced Language Model：利用Gemini架构，在一个更大更多样的数据集上训练。

# More general domain language

首次提出是在AG1中，AG1使用一个简单的领域专用的语言，包含了九个基本的“谓词”，在Table 1中列出。当这些谓词足够覆盖所有2000-2024 IMO中66%的几何问题时，AG1的语言并不允许讨论线性方程、点/线/圆的移动、或一些常见问题，比如“求角度……”。下表展示了作者的AG2如何解决这些和其他挑战。

| Name             | Meaning                                                       |
| :--------------- | :------------------------------------------------------------ |
| cong a b c d     | $AB = CD$                                                     |
| perp a b c d     | $AB \perp CD$                                                 |
| $\dots$          | $\dots$                                                       |
| aconst a b c d x | Angle Between AB and CD is equal to x, where $x \in [0, 180)$ |
| rconst a b c d y | AB:CD = y where y is a constant                               |

上述Table 1是AG1的谓词。

首先，AG2新增了两个谓词来允许问题类型：“求x”：

1. `acompute a b c d` 表示：求解AB和CD之间的角度。
2. `rcompute a b c d` 表示：求解AB/CD的比例。

在一些几何问题中，包括出现在IMO 2024中的那道题目，存在几何量的线性方程(角度，距离)，而这些是AG1不能捕捉到的。为了表达这些想法，AG2增加了下述的三个谓词：

1. `distmeq a1 b1 a2 b2 ... an bn t1 t2 ... tn y` 表示：$t_1 \log(A_1 B_1) + t_2 \log(A_2 B_2) + \dots + t_n \log(A_n B_n) + y = 0$
2. `distseq a1 b1 a2 b2 ... an bn t1 t2 ... tn` 表示：$t_1 A_1 B_1 + t_2 A_2 B_2 + \dots + t_n A_n B_n = 0$
3. `angeq a1 b1 a2 b2 ... an bn t1 t2 ... tn y` 表示：$t_1 d(A_1 B_1) + t_2 d(A_2 B_2) + \dots + t_n d(A_n B_n) + y = 0$，其中 $d(AB)$ 是无向线段AB和水平线之间的角度。

另一类AG1不支持的类别是，所谓的轨迹问题，主要描述的是如点、线、圆的对象的运动。AG2通过一个新的谓词语法捕捉这种运动。Table 2列出了11个轨迹的例子，以及相关的谓词和它们的语法。此处，作者利用了新的标记 `*` 来充当固定点的占位符。

进一步地，在AG2的证明中，作者引入了明确地谓词，来表示拓扑 / 非退化条件的几何图检查。

1. `sameclock a b c d e f` 表示：方向 $A \to B \to C$ 与 $D \to E \to F$ 有同样的顺时针方向。
2. `noverlap a b` 表示：A和B是不同的点。
3. `lessthan a b c d` 表示：$AB<CD$ ，是一个在SSA的三角同余定理中使用的声明。

AG2也能通过引入新的谓词，来证明点是相同的，`overlap a b` (点A和B是重合的点)，其中任何涉及A点的谓词也能够被用于B点，反之亦然。在推理闭包中，重合点可以通过成为同一个圆的圆心来定义；因此，作者引入了另一个谓词 `cyclic_with_center` 来捕捉这种情况。此处，`cyclic_with_center a1 a2 ... an x` 表示 $a_1 = a_2 = \dots = a_x$ 是圆的圆心，直到 $a_{x+1 \dots a_n}$ (以免x=0，它也等价于 `cyclic`)。

>Notice that, when describing a problem, AG1 uses at most 2 predicates to define a point, i.e. each point is defined as the intersection between at most two objects (line or circle).

注意到，当描述一个问题时，AG1使用最多2个谓词来定义一个点，也就是说，每个点被定义为至多两个对象(线或圆)的交点。

>This limits AG1 to only constructive problems - problems where all points can be straightforwardly constructed by following their definition order and taking the intersection of two well-defined objects.

这限制了AG1仅能解决构造型问题，这些问题中所有的点都是由他们定义顺序，以及用两个已经完整定义好的对象的交点来定义。

在AG2中，作者放宽了这个约束，来覆盖更多的问题，那些问题中能够由最少三个谓词来定义，使得图形的构造不再平凡(non-trivial)。作者用于自动化该过程的方法将在下一节讨论。

在该部分中描述的所有更新，提高了AG语言的覆盖率，根据2000-2024年所有IMO的几何问题，从66%到88%。剩余的12%包含3D几何、不等式、非线性方程，以及可数的但很多的点(也就是那些含有n个点的问题，而n是一个任意的正整数)。所有被AG1和AG2覆盖或没覆盖的问题，都可以在Figure 8中找到。那些没有被覆盖的问题都被标为“Not attempted”。

# Automated problem formalization and diagram generation

## Automated formalization

AG以及其他类似的神经-符号系统的一个很大的弱点，是需要将输入的问题，从自然语言人工翻译成一种领域内专用的语言。例如，一道简单的以自然语言描述的几何问题：“给定一个三角形ABC，边AB=AC，证明角B等于角C。”，会被翻译成：`triangle a b c; a b = a c ? eqangle b a b c c b c a` ，这才是AG专用的语言。

将这个过程自动化之后，就是所谓的形式化(formalization)，这是一个非常热门的研究领域。这相比于人类语言之间的翻译，是一个复杂得多的问题。翻译只是为了保持原意，而形式化则频繁地需要将原问题重新公式化(re-formulating)为一种可以替换的形式，而且有时候还需要剔除原本题目声明中那些存在二义性的细微差别(disambiguating the nuances)。

自动形式化(auto-formalization)，因而需要极强的背景知识，以及本身就需要的问题求解能力。考虑到最近的基础模型开始展现出这种能力，作者也就使用了一个这类的模型，Gemini，来自动为AG完成形式化的工作。

作者开始是手工将几十个几何问题翻译成AG语言，然后利用这些例子来撰写一个少样本提示词(few-shot prompt)，要求Gemini将一道给定的几何问题，从自然语言翻译成AG语言。作者使用该提示词问询Gemini五次，然后再次调用Gemini，要求将这些结果合并为一个最终的答案。通过这种方法，作者得以形式化39道可形式化的IMO 2000-2024几何问题中的30道。对于更加简单的几何问题，它的表现非常稳定，而且几乎不会犯错。

## Automated diagram generation

另一个作者提出的主要管线中的人工部分，则是图形生成。在AG1中，每个点都由Table 1中至多两个基本的谓词所定义，因此问题都是构造性地定义的，而且图形也能被自动地生成。

>In AG2, we allow one or multiple points being defined simultaneously by an arbitrary number of predicates, allowing us to also cover non-constructive problems.

在AG2中，作者允许一个或多个点被任意数量的谓词同时定义，这就允许我们也能够覆盖那些非构造性的问题。

考虑一个非构造性问题的声明：“令ABC是一个含有内心I的三角形，使得$IA = 2IB$”，此处的I点不仅被定义为了内心，也就是两条内角平分线的交点，还被第三个谓词“$IA = 2IB$”所定义，而且并没有通用的能够构建这四个点的策略。既然AG2覆盖了那些非构造性的问题，图形构建变成了主干中一个重要的部分，而且通常需要人为干预。与Kruger (2021)的工作类似，作者提出了一下的算法，在给定非构造性问题要求的情况下，自动生成图形。

令 $\hat x \in R^{2n}$ 是一个向量，表示所有点的所有坐标。作者将图形中的所有约束 $c$，包括目标，都编码成 $f_c(\hat x) = 0$，其中 $f_c$ 是非线性函数。作者用两步来在数值上搜索一个合适的 $\hat x$。

首先，作者会用ADAM剃度下降优化，在均方误差损失函数 $\Sigma_{c \in C} f_c (\hat x)$ 上运行ADAM剃度下降优化，其中 $C$ 是所有约束的集合，再加上一个非退化性损失。对于每两个点A, B，作者会加上形如 $1/(|AB|^2 + \epsilon)$ 的损失函数，以及一个 $L_2$ 规范化会被加在所有的点上，以避免它们的值变得过大。

在ADAM优化过后的损失达到一定的阈值之后，作者就不再考虑那些非退化性情况了，并且从剃度下降优化转换到一个高斯-牛顿-列文伯格方法(Gauss-Newton-Levenberg)，来寻求一个联合欠定和超定(under- and over-determined)非线性方程组的数值解。

该两阶段优化方法建立在 Krueger (2021) 提出的方法论上。当第一阶段保持不变时，作者会融入一个全新的第二阶段。该新增的阶段，解决了在原始方法中对梯度下降优化进行调整时所遇到的实际限制问题，而在原始方法中，要实现始终令人满意的误差范围，是颇具挑战性的。

作者在44道形式化为AG语言的IMO题目上，对该方法进行了基准测试(见 Figure 8)，并且能够找到其中41道的图形。作者在多个并行进城中，运行了该两阶段的收敛程序，而且在一个循环中进行，该循环在失败后能够重启，并生成另一个随机的初始构型。通过这种方法，40/44的问题能够在一个小时内，生成它们的图形，每道题大概使用40个进程(许多问题能够在初次尝试时，在几秒钟内就可以得到它的图形)。对于剩下的4道题，作者用更长的时间以及更大的并行量，运行了相同的程序。通过这种方式，作者在3333个进程上，运行了400分钟后，也成功绘制出了IMO-2011-6的图形。

# Stronger and faster symbolic engine

所谓符号引擎，是AG的核心组件。作者称之为DDAR，也就是推理数据库(Deductive Database)和算数推理(Arithmetic Reasoning)。它是一个推理闭包的算法，也就是在给定一个核心初始结论(facts)集合的情况下，得到的推理结论(facts)的集合。DDAR根据一个固定的推理规则集合，来构建这个推理闭包，迭代地将新的结论(facts)加入到这个推理闭包中，直到没有更多能添加的了。

DDAR既驱动训练数据生成，用于作者地语言模型，也在测试阶段的证明搜索中指导了推理步骤的搜索。在这两种情况中，速度是关键。更快的数据生成允许更大且更激进的数据过滤，同时更快的证明搜索又能促使更广泛的搜索，这个过程在一个给定的时间限制内，增加了找到一个解答的可能性。

在接下来三部分中，有三个DDAR的主要改进会被讨论。

- 处理“重合点”的能力
- 更快的算法
- 更快的实现

## Handling double points

当重新实现DDAR时，作者试着去保持与原算法，大致相同的逻辑强度，只因实现的不同变得略长一些(例如：Thales定理被更加通用的圆心角定理所替换)。然而，DDAR1缺少了一个关键的特征，对于处理有难度的问题是很关键的：它不能接受两个点有相同的坐标，但是却有不同的名称。

例如，想象一个问题是：两条线a, b相较于点X，需要证明X处在一个确定的圆 $\omega$ 上。最合理的方法可能是通过一种重新阐述的方法(reformulation)——与其证明a, b的交点在 $\omega$ 上，我们可以证明 a, $\omega$ 的交点在b上。这两个问题是等价的，(后者)却更容易证明，因为我们可以在圆上移动角度。

可以见Figure 1，为了在推理“重合点”问题时，实现“重新阐述”，作者通过接下来的4步来完成这个工作：

- 构造一个新的点 $X'$ 作为 a, $\omega$ 的交点（我们暂时还不知道 $X'$ 会与 $X$ 点重合）。该辅助构造必须由一个语言模型预测出来。
- 证明 $X$ 在 $b$ 上。
- 既然 $X$ 和 $X'$ 都处在 a, b 上，我们可以推出：$X=X'$ 。
- 因此，$X$ 在 $\omega$ 上。

## Faster algorithm

>The DDAR1 algorithm is processing a list of rules, and tries to apply each rule to all combinations of points.

DDAR1 算法处理一个规则列表，并且试着将每一条规则，运用到所有的点的组合上去。

该过程包括一个候选搜索步骤，其时间复杂度与点的个数呈多项式关系，以及一个子句匹配步骤，其时间复杂度与每个前提的子句数量呈指数关系。理论上，在AG1中搜索相似角的候选角，最差情况下的复杂度是 $O(N^8)$，几乎是最耗时的步骤之一。指数级增长的子句匹配，则是另一个十分耗时(expensive)步骤。为了让搜索更加高效，作者提取了所有核心规则，并对其应用进行了硬编码搜索，从而降低了对AR子引擎进行查询的次数，降低到最多立方次的(cubic)数量级水平。更进一步的，作者抛弃了用于角度和距离的，比较显而易见的规则(例如，关于垂直或平行线的)——所有类似的推理都自动地在AR引擎中发生。

DDAR中的两个主要耗时部分，一个是一次对相似三角形的搜索，另一个是一次对循环四边形的搜索。在AG2中，作者设计了一个改进的DDAR2算法。

对于相似三角形，系统会认真检查所有点的三元组，哈希存储它们的“形状”，并在形状被二次识别时，检测到相似的一对。

对于循环四边形，系统会遍历所有对(点X，线段AB)，并且将 $(A, B, \angle AXB)$ 的值哈希存储。如果类似的三元组重复了，我们就得到了一个循环四边形。通过线段AB、或 $\angle AXB$ 的“值”，此处的值作者指的是，一个由AR子模型计算得到的符号标准形式。该子模型会跟踪那些已知的，关于角度、距离、以及对数距离的线性方程，理解它的代数结果，并且能够将任何线性表达式简化成它的标准形式。

## Faster implementation

尽管新的算法已经显著地加速DDAR，作者仍然通过在C++中实现它的核心计算(高斯消元法，Gaussian Elimination)来进一步加速。新的C++库，通过pybind11导出到Python中，比DDAR1要快300倍。为了对速度的提升进行基准测试，作者选取了25道DDAR无法解决的IMO问题，然后在一台有AMD EPYC 7B13 64 core CPU的计算机上运行测试50次。尽管DDAR1平均在1179.57 $\pm$ 8.055秒内完成了，但是DDAR2要快得多，在3.44711 $\pm$ 0.05476秒内完成计算。

# Better synthetic training data

使用一个语言模型来辅助符号引擎，是AG1成功的关键，在挑选出的30道IMO问题中，将解出率从14道题(纯推理证明)提升到25道。该语言模型是在海量的，由算法自动升成的合成数据上进行训练的。在AG2中，作者使用了相似流程。

与AG1类似，作者的合成数据生成方法，从采样一个随机图形开始，并使用符号引擎来从中推理出所有可能的结论。对于每一个推理出的结论，一个回溯算法被用来提取出相关的前提、辅助点、以及那些能够证明出结论的推理步骤。

作者地数据升成方法有意地避开了将人类精心构造的问题，用于初始的图形种子，并严格地从随机图形开始。这种设计选择消除了数据污染的风险，而且使得对定理分布的探索变成了可能，因为定理可能会扩展到超越已经完善构建的人类知识。

该方法与TG的方法不同，TG依赖于人类的专家知识，以及现有的问题图形来引导，并且过滤数据生成。在AG2中，作者坚持使用随机图形作为初始种子，并力争获得更好的合成训练数据。

## Larger, more complex diagrams and better data distribution

首先，作者扩大了数据生成的资源，并且做了更加仔细的数据分布的重平衡(re-balancing)。具体数据在Figure 2中有展示，对比了AG2与AG1：

- 以两倍大小探索锁机图形，允许提取出更加复杂的问题。
- 生成至多两倍复杂程度的定理，也就是点和前提的数量(的两倍)。
- 生成至多10倍更加复杂的证明，也就是10倍多的证明步骤。
- 针对问题(question)类型，有更加平衡的数据分布。
- 针对是否需要辅助点的问题，有更加平衡的数据分布。

## More types of theorems

除了生成定理，来证明经典命题，如“$AB = CD$”之外，AG2的数据生成算法也能生成“轨迹型(locus)”的问题，也就是断言如：“当X在线/圆Y上移动时，Z在一个固定的线/圆T上移动”。在Section 2中有介绍，这些命题并没有被AG1的数据生成算法所支持，因为并没有运动或者运动依赖(movements dependencies)。在AG2中，通过函数 $P(.)$ 做随机图形生成时，系统会记录每个点X的移动依赖。函数定义如下：

- $P(A)$：控制A运动的点的集合，其中A是一个点或一个点的集合，以一个构造性问题的声明方式定义。两个P的例子列在Table 3中，而那些被检测为轨迹型命题的所有案例，都展示在Table 5中。

Table 3，展示了P的两个例子：

- 第一行：既然d已经被单独地定义为a和c的中点，a被单独地定义为b和c地中点，d点移动的来源就是b和c。即：`If a=midpoint b c, d=midpoint a c, Then P(d)={b,c}`。
- 第二行：因为a可以出现在线段bc上的任何位置，a自己就是移动来源的一部分。即：`If a=onl_line b c, Then P(a)={a,b,c}`。

## Faster data generation algorithm

坐着也提升了数据生成算法的速度。回忆AG1中的算法，作者首先在随机图形上运行了推理闭包，并且“回溯”来获得最小的问题和最小的，能够证明闭包中每个结论的证明。为了获得AG1中的最小问题，作者不得不不知疲倦地(exhaustively)从题目中去除点的不同的子集，并重新运行DDAR来检查可证明性(provability)。这样的搜索可以以最小的基数来找到子集，但是作为一个指数级增长的搜索来说，对于更大数量的点，这种方法就不可行了。

因此，作者更换成了Figure 3中所示的一种贪婪丢弃算法(greedily discarding algorithm)，该算法仅用线性数量的检查，即可判断一组点是否足以证明出目标。贪心算法能保证找到一个关于包含关系而言，最小的点集，只要检查是单调的(monotonic)。(如果 $A \subseteq B$, 那么 `check_provable(A)` $\Rightarrow$ `check_provable(B)`)。

实际上，作者也需要修剪过的集合，来在构造依赖下，保持闭包的状态(closed)。（这样的话，我们就仍然可以进行一个随机的构造）。如果我们将这个条件用于 `check_provable` 谓词中，它就不再单调了。这个难题可以通过按照逆拓扑顺序处理的，图3中的算法来解决（首先处理不依赖于任何其他点的点，最后处理构造的初始点）。

Figure 3：展示了一个基本的贪心算法，用于找到满足单调谓词检查的，最小的，点的集合。

```python
def prune_points(points: set[Point], check_provable: Callable[set[Point]], bool):
	pruned = set(points)
	for p in reverse_topological(points):
		if check_provable(pruned - {p}):
			pruned = pruned - {p}
	return
```

# Novel search algorithm

在AG1中，作者使用了一个简单的束搜索来发现证明。在AG2中，作者设计了一种全新的搜索算法，其中，几个不同配置的束搜索都并行执行，并且通过一个知识共享机制来互相帮忙(见Figure 4)。

>To improve the robustness of our system we use multiple different language models for each search tree configuration. We call this search algorithm Shared Knowledge Ensemble of Search Trees (SKEST).

为了提升系统的鲁棒性，作者采用多个不同的语言模型，来搜索树的构型。作者把这个搜索算法称为：搜索树的共享知识集成。

这个算法是这样工作的。在每个搜索树中，一个节点与一个辅助构造的尝试相关，紧接着的是符号引擎的一次运行尝试。如果该尝试成功了，那么所有的搜索树都会终止。如果该尝试失败了，该节点会将符号引擎成功证明地结论，写入到一个共享的结论数据库中。这些共享的结论会被过滤，从而，它们不再是特定于该节点的辅助点，而仅仅是与初始问题相关。通过这种方式，这些结论对于相同搜索树中的其他节点来说，也能派上用场。作者也是通过这种方式，来确保搜索空间中的不同部分，都被有效地探索过了：

- “经典的”搜索树：与AG1中所使用的集束树搜索相同，一个语言模型会被要求在每个节点上，生成一个辅助点。
- 树会对每个节点都预测多个辅助点：语言模型在每个树的节点上，被允许生成它想要的，尽可能多的辅助点。回想一下，这种操作是可能的，因为作者的语言模型被训练成生成完整证明，从辅助点开始，紧接着是推理步骤。需要注意，尽管作者想要模型在一次查询中，生成所有必要的辅助点，实际上他们观察到，在给定先前生成辅助点的情况下，还是有必要去多次调用模型。允许模型升成多个辅助点，加速了找到一个解答的时间，而且还有效地增加了树的搜索深度。
- 树会统一地预测不同类型的辅助点。会想语言模型对于辅助点的输出形如：`x00 a : cong a b c d (00) coll a e f (01)`，也就是：“构造点a满足ab=cd，而且a e f是共线的”。通常要预测辅助点，作者会用开始符号 `x00` 来提示语言模型，并让其生成剩余部分。此处，取而代之的是，作者用 `x=00 a : cong`, `x=00 a : coll`, `x=00 a : cyclic`, `x=00 a : perp` 等等提示词来提示模型，迫使其对于前面四个符号是均匀分布的，然后再让语言模型升成剩余的部分。
- 更深但是更窄的树，例如集束大小64，深度10。
- 更浅但更宽的树，例如集束大小512，深度4。

## System design details

对于证明搜索，作者使用 TPUv4 来服务每个模型的多重副本，并让在相同模型中的不同搜索树，在它们自己的搜索策略下，对同一个服务器进行查询。除了异步地运行这些搜索树之外，作者还异步运行语言模型的工作进程与DDAR的工作进程。



# Better language model

最后一个AG2的改进是一个新的语言模型，在这部分，作者会讨论他们全新的训练和推理设置。

## Training setup
