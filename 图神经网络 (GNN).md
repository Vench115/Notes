# 视频资源

[零基础多图详解图神经网络（GNN/GCN）【论文精读】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1iT4y1d7zP/?spm_id_from=333.880.my_history.page.click&vd_source=5c0fccc3e62acb7264f3cd05395b00c0)

[【GNN 图神经网络】直观透彻理解_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1nu411e7yb/?spm_id_from=333.788&vd_source=5c0fccc3e62acb7264f3cd05395b00c0)

# 博客文章

[A Gentle Introduction to Graph Neural Networks (distill.pub)](https://distill.pub/2021/gnn-intro/)

# 引入

> 关于GNN...

## 如何处理数据

1. 把数据表示成图
2. 后一层的顶点由前一层它的邻居计算而来，图的层数越深，结点就能处理很大范围的信息
3. 定义问题

## 如何定义问题

1. 对图进行分类（可以用正常的编程语言解决；对于复杂问题，可以借助GNN）
2. 顶点层面的任务，类似聚类
3. 边层面的任务，例如语义分割后，预测每一条边上的属性是什么

## 挑战

1. 如何表示**图**，使得其与神经网络是**兼容**的。
2. 矩阵可能非常巨大(n个顶点有n\*n)，就需要压缩。
3. 想要高效地计算稀疏矩阵是一个很难的问题，尤其是在GPU上计算稀疏矩阵时。
4. 图的顶点顺序交换，图会变化，但是神经网络需要保证结果不变。
5. 如何用神经网络来处理。

# 如何存储

> 要求：存储高效且顶点顺序无关(Invariant to Node-Order)

## 点 Nodes
每个点使用的属性使用**标量**来表示，也可以使用**向量**来表示
## 边 Edges
每条边也使用标量来表示，同时也可以使用向量
## 全局信息 Global Context
全局信息可以用标量，也可以用向量来表示
## 邻接列表 Adjacency List
- 该邻接列表的**长度**与**边**的数量相同
- 第i个项表示的是第i条边连接的是哪两个节点
- 存储上，只存了边和属性，所以较为**高效**
- **顺序无关性**：
	- 因为可以将边的顺序任意打乱，只需要将邻接列表的边的顺序也相应打乱就可以了；
	- 同样的，对于顶点顺序，也可以用相似的操作（更新邻接列表中对应的数字）来保证点顺序无关性（Node-Order）

# Graph Neural Networks

## 定义

> **A GNN is an optimizable transformation on all attributes of the graph (nodes, edges, global-context) that preserves graph symmetries (permutation invariances).**

GNN是一个对图上所有的属性(attributes)，包括顶点、边和全局的上下文(context)，进行的一个可以优化的变换(transformation)，这种变换可以保持住图的对称信息。

## 对称信息 (symmetries)

当我们把这些顶点进行另外一种排序(打乱顶点顺序)之后，整个GNN对于变换过后的图的处理结果是保持不变的。

## 信息传递神经网络

"Message Passing Neural Network"框架，当然GNN也可以用别的方式来表述。

## 输入 / 输出

秉持着"graph-in, graph-out"的理念，GNN的输入是一个图，输出也是一个图。

它会对那些属性(embeddings)，也就是顶点、边、和全局的那些向量进行变换。但是它并不改变图的连接性(connectivity)，也就是哪些边邻接了哪些顶点的信息，在进入GNN后是不会被改变的。

# 一个最简的 GNN

## 构建方法

对于所有的全局信息向量、顶点向量、边向量，我们都分别构造一个MLP(Multi-Layer Perceptron)，也就是多层感知机（相当于全连接层）。

这个MLP的输入和输出的大小都是一样的，取决于输入的向量。这三个MLP就组成了一个GNN的层(Layer)，该层的输入和输出都是一个图(结构)。

## 层的作用

对于顶点、边、全局的向量，分别找到对应的MLP，把这些向量放入MLP，然后输出，并作为它们各自对应的更新。

1. 这些属性被更新过以后，图的结构没有发生变化，即满足了第一个要求：只对属性进行变换，但是不改变图的结构。
2. 因为MLP是对每一个向量独立作用的，而不会考虑所有的连接信息，所以，在此处不论对顶点做任何排序或其他操作，都**不会改变结果**。

至此，这个最简单的GNN层也就满足了之前提到的要求。当然，我们可以**叠加这些层**，以构造一个比较深一些的GNN。

# 预测 (Prediction)

> 最后一层的输出如何得到我们需要的预测值呢？

## 对顶点做预测

与一般的神经网络没有太多区别，因为顶点已经有了一个向量的表示。

所以我们只要在向量后增加一个输出维度（预测值）的全连接层，再加一个Softmax函数就可以得到我们需要的输出了。

若给出的是最后一层的输出（一个图），然后对于每一个顶点，将其放入全连接层，再得到输出，如此得到对于顶点的分类。

**注意：**
跟之前一样，不论有多少个顶点，此处都只有一个全连接层。也就是说，所有的顶点都会**共享**一个全连接层里面的参数。

在之前的GNN层中，不论图有多大，都是三个MLP，所有的顶点、边、全局信息都共享一个MLP。当然，这是因为这里提到的情况，是GNN中**最简单的**情形。

## 略微复杂的情形

假设：我们仍然想要对顶点做预测，但是没有顶点向量，该如何处理？
- 此时，我们会使用到一个技术叫作**Pooling**，称为**汇聚**。
- 在CNN中，我们其实已经见过它了，与此处的Pooling没有什么本质上的区别。

我们可以将与这个点**相连的边**的向量都拿出来，同样把**全局**的向量拿出来，然后把这些向量全部**加起来**，就会得到代表这个顶点的向量。有了这个向量，我们就可以通过最后的全连接层来得到输出。

此处假设顶点、边、全局的向量维度都是一样的，如果不一样，则需要做一些投影操作。

## 对边进行预测

假设我们没有边的向量信息，但是有顶点的向量，此时想要对边进行预测。那么同样道理，我们可以把顶点的向量**汇聚**到边上。

一条边连接两个顶点，然后这两个顶点向量我们可以把它们加起来，或者再加上全局的那个向量，来得到这个边的向量。然后进入边向量的输出层，也就是所有的边共享一个输出层，最后得到这个边的输出。

## 对整个图做预测

假设我们没有全局的向量，但是有顶点的向量，目标是对整个图做预测。

我们可以将所有顶点向量加起来，得到一个全局的向量，然后进入全局的输出层，并得到一个全局的输出。

总的来说，不论我们缺乏哪一类的属性，我们都可以通过汇聚（Pooling）这个操作，得到我们需要的那个属性的向量，最终产生我们的预测值。

## 总结

最简单的GNN如何构成？

给出一个输入的图，首先进入一系列的GNN层，每个层中有三个MLP（多层感知机），分别对应三种不同的属性（点、边、全局）。

逐一经过这三个MLP后，会得到最终的输出，其保持了整个图的结构，但是里面所有的属性已经发生了变化。最后根据我要对哪一个属性进行预测，再添加合适的一些输出层，如果我们是缺失信息的话，我们就加入合适的汇聚层，就可以完成我们要的预测了。

## 局限性

虽然这个情况很简单，但是我们可以发现这种方法有很大的局限性。

主要的问题在于经过GNN层时，我们**并没有**使用图的**结构信息**，只是每个属性进入自己的MLP。

也就是说，我们没有利用到比如：这些顶点是与哪些边相连的，或者与哪些顶点是相连的；一条边是连接了哪些顶点，以及跟另外哪些边是相连的等等。

所有类似于这样的连接信息，我们都**没有**考虑到，这导致了最终的结果可能并不能够特别充分利用(leverage)整个图所包含的信息。

# 信息传递 Passing Messages

为了一定程度上改善上述的局限性，我们使用到了名叫**信息传递**的技术，与之前提到的**汇聚**的思想上有些许相似。

## 工作方式

在这里提及的信息传递中，我们不仅仅把自己的向量加入更新，我们还要将与这个点相邻的那些顶点，也加入到这个**汇聚**中来，得到一个新的汇聚的向量。然后，我们再把这个向量放入MLP中，就能得到我这个点的向量的更新。

## 卷积的思考

作者表示，这种方式其实跟标准意义上的**卷积**有点像。如果严格意义上讲，在这个例子上，那些卷积核中窗口中的权重应该是一样的。

因为在卷积当中，我们其实是对每一个顶点和它的邻居顶点的向量，都做一个加权和，这里的权重来自于卷积核窗口中的每一个像素自己的权重。

但是在图网络中的简单设计中，我们就没有加权和了，所以导致那些权重都是一样的，但是通道还是保留了。

如果我们每一层只考虑一个顶点和它的邻居的关系，但是如果有很多层放在一起的话，最后一层的顶点会把很多图中的信息都汇聚进来，所以就完成了图的比较长距离的一个信息传递的过程。

# 改进预测

## 顶点层面

在顶点之间，把距离为1的邻居的信息都传递过来，就是一个汇聚的过程。

当然，这个任务也可以更复杂一些，假设我们缺失了一些信息，那我们可以从别的属性汇聚过来，弥补我们缺失的属性。

同样的，这种操作可以不用等到最后再进行，而是在比较早的时候，就进行边和顶点之间的信息汇聚。

## 顶点与边

可以先把顶点的信息传递给边，然后再把边的信息传回给顶点。

# GNN Playground

作者在这个篇幅中，写了一个程序来模拟图神经网络的调参过程，并且让读者能够调节其中得到超参数，并观测结果的变化。但是这种简单的操作并不能让我们学习到一些经验，所以作者直接写了一些经验上的内容，并做了一些统计图来直观地表述，超参数所能带来的变化。

# 相关技术介绍

> Into the Weeds.

## 其他类型的图

multigraphs, hypergraphs, hypernodes, hierarchical graphs

## 采样与批量计算

因为计算梯度需要保存图的中间结果，所以我们希望能够降低计算压力，这就需要用到**采样 Sampling**，那么在图的结构上做采样是什么样的呢？

随机选点与1近邻采样策略、随机游走采样策略、随机游走与1近邻结合采样策略、扩散点采样策略。

## 归纳偏置

Inductive biases

我们需要有对世界的假设，这个假设在CNN上假设平移不变性，在RNN中是时序的连续性。

那么，在GNN中，这种假设是置换不变性，或者说点序无关性。

## 对比聚合操作

无论是求和、取平均值或者求最大值，都无法区分两个网络的输入，或许我们需要更加好的设计，来做到从聚合操作上区分网络的输入。

## GCN作为子图

不理解，需要听课学习。

## 图卷积神经网络

作者从头到尾都没有说过什么是GCN，它其实就是Graph Convolution Networks.

## 图注意力网络

Graph Attention Networks，需要学习。

## 生成模型

Generative Models
