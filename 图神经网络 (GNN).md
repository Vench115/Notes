# 视频资源

[零基础多图详解图神经网络（GNN/GCN）Bilibili](https://www.bilibili.com/video/BV1iT4y1d7zP/?spm_id_from=333.880.my_history.page.click&vd_source=5c0fccc3e62acb7264f3cd05395b00c0)

[【GNN 图神经网络】直观透彻理解 Bilibili](https://www.bilibili.com/video/BV1nu411e7yb/?spm_id_from=333.788&vd_source=5c0fccc3e62acb7264f3cd05395b00c0)

# 博客文章

[A Gentle Introduction to Graph Neural Networks (distill.pub)](https://distill.pub/2021/gnn-intro/)

# 引入

> 关于GNN...

## 如何处理数据

1. 把数据表示成图
2. 后一层的顶点由前一层它的邻居计算而来，图的层数越深，结点就能处理很大范围的信息
3. 定义问题

## 如何定义问题

1. 对图进行分类（可以用正常的编程语言解决；对于复杂问题，可以借助GNN）
2. 顶点层面的任务，类似聚类
3. 边层面的任务，例如语义分割后，预测每一条边上的属性是什么

## 挑战

1. 如何表示**图**，使得其与神经网络是**兼容**的。
2. 矩阵可能非常巨大(n个顶点有n\*n)，就需要压缩。
3. 想要高效地计算稀疏矩阵是一个很难的问题，尤其是在GPU上计算稀疏矩阵时。
4. 图的顶点顺序交换，图会变化，但是神经网络需要保证结果不变。
5. 如何用神经网络来处理。

# 如何存储

> 要求：存储高效且顶点序无关(Invariant to Node-Order)

## 点 Nodes

每个点使用的属性使用**标量**来表示，也可以使用**向量**来表示
## 边 Edges

每条边也使用标量来表示，同时也可以使用向量
## 全局信息 Global Context

全局信息可以用标量，也可以用向量来表示
## 邻接列表 Adjacency List

- 该邻接列表的**长度**与**边**的数量相同
- 第i个项表示的是第i条边连接的是哪两个节点
- 存储上，只存了边和属性，所以较为**高效**
- **顺序无关性**：
	- 因为可以将边的顺序任意打乱，只需要将邻接列表的边的顺序也相应打乱就可以了；
	- 同样的，对于顶点顺序，也可以用相似的操作（更新邻接列表中对应的数字）来保证点顺序无关性（Node-Order）

# Graph Neural Networks

## 定义

> **A GNN is an optimizable transformation on all attributes of the graph (nodes, edges, global-context) that preserves graph symmetries (permutation invariances).**

GNN是一个对图上所有的属性(attributes)，包括顶点、边和全局的上下文(context)，进行的一个可以优化的变换(transformation)，这种变换可以保持住图的对称信息。

## 对称信息 symmetries

当我们把这些顶点进行另外一种排序，也就是打乱顶点顺序之后，整个GNN对于变换过后的图的处理结果是保持不变的。

## 信息传递神经网络

"Message Passing Neural Network"框架，当然GNN也可以用别的方式来表述。

## 输入 / 输出

秉持着"Graph-in, Graph-out"的理念，GNN的输入是一个图，输出也是一个图。

它会对那些属性(embeddings)，也就是顶点、边、和全局的那些向量进行变换。但是它并不改变图的连接性(connectivity)，也就是哪些边邻接了哪些顶点的信息，在进入GNN后是不会被改变的。

# 一个最简的 GNN

## 构建方法

对于所有的全局信息向量、顶点向量、边向量，我们都分别构造一个MLP(Multi-Layer Perceptron)，也就是多层感知机（相当于全连接层）。

这个MLP的输入和输出的大小都是一样的，取决于输入的向量。这三个MLP就组成了一个GNN的层(Layer)，该层的输入和输出都是一个图(结构)。

## 层的作用

对于顶点、边、全局的向量，分别找到对应的MLP，把这些向量放入MLP，然后输出，并作为它们各自对应的更新。

1. 这些属性被更新过以后，图的结构没有发生变化，即满足了第一个要求：只对属性进行变换，但是不改变图的结构。
2. 因为MLP是对每一个向量独立作用的，而不会考虑所有的连接信息，所以，在此处不论对顶点做任何排序或其他操作，都**不会改变结果**。

至此，这个最简单的GNN层也就满足了之前提到的要求。当然，我们可以**叠加这些层**，以构造一个比较深一些的GNN。

# 预测 Prediction

> 最后一层的输出如何得到我们需要的预测值呢？

## 对顶点做预测

与一般的神经网络没有太多区别，因为顶点已经有了一个向量的表示。

所以我们只要在向量后增加一个输出维度（预测值）的全连接层，再加一个Softmax函数就可以得到我们需要的输出了。

若给出的是最后一层的输出（一个图），然后对于每一个顶点，将其放入全连接层，再得到输出，如此得到对于顶点的分类。

**注意：**
跟之前一样，不论有多少个顶点，此处都只有一个全连接层。也就是说，所有的顶点都会**共享**一个全连接层里面的参数。

在之前的GNN层中，不论图有多大，都是三个MLP，所有的顶点、边、全局信息都共享一个MLP。当然，这是因为这里提到的情况，是GNN中**最简单的**情形。

## 略微复杂的情形

假设：我们仍然想要对顶点做预测，但是没有顶点向量，该如何处理？
- 此时，我们会使用到一个技术叫作**Pooling**，称为**汇聚**。
- 在CNN中，我们其实已经见过它了，与此处的Pooling没有什么本质上的区别。

我们可以将与这个点**相连的边**的向量都拿出来，同样把**全局**的向量拿出来，然后把这些向量全部**加起来**，就会得到代表这个顶点的向量。有了这个向量，我们就可以通过最后的全连接层来得到输出。

此处假设顶点、边、全局的向量维度都是一样的，如果不一样，则需要做一些投影操作。

## 对边进行预测

假设我们没有边的向量信息，但是有顶点的向量，此时想要对边进行预测。那么同样道理，我们可以把顶点的向量**汇聚**到边上。

一条边连接两个顶点，然后这两个顶点向量我们可以把它们加起来，或者再加上全局的那个向量，来得到这个边的向量。然后进入边向量的输出层，也就是所有的边共享一个输出层，最后得到这个边的输出。

## 对整个图做预测

假设我们没有全局的向量，但是有顶点的向量，目标是对整个图做预测。

我们可以将所有顶点向量加起来，得到一个全局的向量，然后进入全局的输出层，并得到一个全局的输出。

总的来说，不论我们缺乏哪一类的属性，我们都可以通过汇聚（Pooling）这个操作，得到我们需要的那个属性的向量，最终产生我们的预测值。

## 总结

> 最简单的GNN如何构成？

给出一个输入的图，首先进入一系列的GNN层，每个层中有三个MLP（多层感知机），分别对应三种不同的属性（点、边、全局）。

所有的属性逐一经过这三个MLP后，会得到最终的输出，其保持了整个图的结构，但是里面所有的属性已经发生了变化。

最后根据我要对哪一个属性进行**预测**，再添加合适的一些**输出层**，如果我们是缺失信息的话，我们就加入合适的**汇聚层**，就可以完成我们要的预测了。

## 局限性

虽然这个情况很简单，但是我们可以发现这种方法有很大的局限性。

主要的问题在于经过GNN层时，我们**并没有**使用图的**结构信息**，只是每个属性进入自己的MLP。

也就是说，我们没有利用到比如：这些顶点是与哪些边相连的，或者与哪些顶点是相连的；一条边是连接了哪些顶点，以及跟另外哪些边是相连的等等。

所有类似于这样的连接信息，我们都**没有**考虑到，这导致了最终的结果可能并不能够特别充分利用(leverage)整个图所包含的信息。

# 信息传递 Messages Passing

为了一定程度上改善上述的局限性，我们使用到了名叫**信息传递**的技术，与之前提到的**汇聚**操作在思想上有些许相似。

## 工作方式

在这里提及的信息传递中，我们不仅仅把自己的向量加入更新，我们还要将与这个顶点相邻的那些顶点，也加入到这个**汇聚**中来，得到一个新的汇聚的向量。然后，我们再把这个向量放入MLP中，就能得到我这个点的向量的更新。

## 与卷积类比

作者表示，这种方式其实跟标准意义上的**卷积**有点相似。如果严格意义上讲，在这个例子上，那些卷积核中窗口中的权重应该是一样的。

因为在卷积当中，我们其实是对每一个顶点和它的邻居顶点的向量，都做一个加权和，这里的权重来自于卷积核窗口中的每一个像素自己的权重。

但是在图网络中的简单设计中，我们就没有加权和了，所以导致那些权重都是一样的，但是通道还是保留了。

如果我们每一层只考虑一个顶点和它的邻居的关系，但是如果有很多层放在一起的话，最后一层的顶点会把很多图中的信息都汇聚进来，所以就完成了图的比较长距离的一个信息传递的过程。

# 预测的改进

## 顶点层面

在顶点之间，把距离为1的邻居的信息都传递过来，就是一个汇聚的过程。

当然，这个任务也可以更复杂一些，假设我们缺失了一些信息，那我们可以从别的属性汇聚过来，弥补我们缺失的属性。

同样的，这种操作可以不用等到最后再进行，而是在比较早的时候，就进行边和顶点之间的信息汇聚。

## 顶点与边

怎样把顶点的信息传递给边，然后再把边的信息传回给顶点。

我们可以通过一个 $\rho_{V_n \rightarrow E_n}$ 把顶点的向量传递给边，就是之前所说的，把每个边连接的两个顶点的信息**融入**自己的向量中。同样，若果维度不一样的话，会做一步投影。

然后，我们再通过一个 $\rho_{E_n \rightarrow V_n}$ ，把它连的边的那些信息也一样地**融入**它自己本身。如果这里我们不想用单纯的加法，也可以使用`concat`操作，也就是把它们并在一起，这也是一种解决方法。

至此，我们完成了顶点→边和边→顶点的信息传递之后，再将属性放入各自的MLP进行更新。

因为之前我们是各自更新各自的，所以没有顺序的问题。但是，在这里我们先融入顶点还是先融入边的方式，会产生不一样的结果。目前，我们没有确定的结论，说谁比谁更好，只是说这两种方法导致不一样的结果。

## 交替更新方案

为了解决上述的问题，我们可以尝试交替更新，就是同时把顶点汇聚到边，以及边汇聚到顶点。汇聚过后，暂时先不“加“，而是先回到各自本身，就导致顶点和边的信息两边都有了，等价于两种方法同时做，向量也会稍宽一些。

## 为何需要全局信息？

之前存在的问题：如果我们每一次只看自己的邻居，假设我们的图真的比较大而且连接没有那么紧密的时候，会导致消息从一个点传递到另一个很远的点时，需要走很长的步数才行。

为了解决上述问题，我们可以加入一个Master Node，或者叫Context Vector。这个点其实是一个虚拟的点，可以跟所有的顶点相连，也跟所有的边相连。这里的“连接”，表示的是抽象的连接，而不是有一条实体的边连接在Master Node与点和边之间。

此处提到的抽象的概念就是U，也就是之前提到的全局信息。

## 全局信息汇聚

这样，我们在汇聚顶点和边的过程当中，我们也可以加入全局信息。最后我们更新自己的U时，也会将所有的边和顶点的信息都融入过来，完成最后的汇聚后，再进行更新。

## 总结

有了上述的消息传递，对于图中的三类属性，我们就都学到了对应的向量。而且这个向量，在早起就已经进行了大量的消息传递。

那么在最后做预测时，我们可以只用本身的向量，也可以把它相邻的那些**边**的向量也拿过来，甚至是把它相邻的那些**顶点**的向量也拿过来，当然，还有一个**全局**的向量。也就是说，我不仅用到了本身的向量，还有其他所有相关的东西都拿过来，一起做预测。

对于这些来自于不同类别的属性，我们可以采用加法，也可以采用concat策略合并在一起。此处，作者提到了这种做法有一点像Attention Mechanism。

也就是说把当前这个query(顶点)相关的东西都拿过来，此处就是利用了图的结构信息，也就可以将那些相近的向量都拿过来使用。

这样，我们就基本了解了基于消息传递机制的图神经网络是怎样工作的。

# GNN Playground

作者在网页中，专门写了一个程序来模拟图神经网络的训练以及测试过程。这个程序可以让读者调节超参数，并观测产生的变化。但是这种简单的操作并不能让我们很好地获得直观经验，所以作者也直接写了一些经验上的内容，并做了一些统计图来直观地表述超参数对最后结果影响的解释。

## 超参数

在作者嵌入的小训练程序中，一共提到了下述的几种可调节的超参数。

- 图神经网络的深度/层数 Depth
- 聚合操作 Aggregation  Function
- 顶点、边、全局的向量大小 Embedding Size

## 参数量大小

表示整个模型可以学习的参数量的大小，即 Numbers of Parameters，与最后结果AUC之间的关系。从作者给出的点图上可以看出，当模型参数变高时，模型的AUC的**上限**是在逐渐增加的。

当然，反过来说，即便模型的参数量很大，**其他参数**也需要调整得当。不然模型的AUC也会下降得比较厉害。在这种情况下，参数量很大的模型跟小模型没有太多本质区别，也就得不偿失了。

## 向量长度

对于点、边和全局属性来说，增加它们的维度(dimensionality)，仅仅是稍微加大模型的AUC平均值，但是总体来讲都不太明显。因为这些属性的改变，模型表现出的方差都比较大。

## 层数

随着模型深度从一层到四层的层数加深，模型精度的平均值在大体上是增加的，但是模型的方差也随之增大。也就是说，在增加深度的同时，我们依然需要微调好**其余的参数**，不然会得到模型参数量很大，但是效果很差的情况。

## 聚合操作

文中提及的聚合操作有三个：求和、取平均、取最大值。

这三种聚合操作在模型精准度的体现上，基本上没有差别。在作者使用的数据集上，这三种操作几乎可以看作是等价的。

## 信息传递方式

这里提到的信息传递，是我们需要在哪些属性之间传递信息，对比不同属性间的信息传递方式，体现模型的效果差异。

从不传递任何信息，也就是那个最简单的GNN，没有任何消息传递，其效果自然也就是最差的。随着我们逐渐增加新的属性来传递信息，直到最后我们在顶点、边以及全局之间都进行消息传递，也就是所有能传递的地方都进行消息传递，此时我们能得到一个较好的模型效果（平均值最高）。

同时，此处我们也需要微调好其他参数，不然会得到比较差的情况。

我们也能注意到，如果我们只传递顶点和全局信息，而不传递边的信息，好像模型的效果也还不错。这意味着那些“原子“之间的”连接键“在消息传递之间好像不那么有帮助。

## 总结

基本上，我们可以得出的结论是：GNN对于超参数是**比较敏感**的，我们能够进行调整的地方也比较多。包括GNN有多少层、嵌入的向量是多大、汇聚操作的选择、信息传递方式的选择等等，这些都会对神经网络产生比较大的影响。

# 相关技术介绍

> Into the Weeds.

原文的最后一个部分，用于讨论一些跟GNN相关的技术话题。

## 其他类型的图

### Multigraph

这类图的顶点之间，可以有多种边，比如两个顶点之间可以有**不同形式**的无向边，或者不同的有向边。

### Hypergraph

分层图，这类图中的顶点其实是一个子图，即hypernode。

### 总结

还有阶层图 hierarchical graphs 等等。当然不同的图，在我们选择如何做信息汇聚的时候，会产生一定的影响。

## 采样 Sampling

### 需求

之前我们讨论过一个问题，假设我们的图有很多层，那么经过多层的消息传递，即便每层只看1近邻，那么最后一层的顶点其实可以看到很大的图，类似于卷积神经网络的感受野。

在这种情况下计算梯度时，我们需要保存整个 forward 里的所有中间变量。那么如果最后一个顶点要看整个图的信息，那么意味着我对它计算梯度时，我们需要把整个图之间的中间结果都存下来。

这种情况所导致的计算量，可能是我们无法承受的。所以我们希望能够降低计算压力，这就需要用到**采样 Sampling**，也就是我们在图上每一次采样一个小图出来，并在这个小图上做信息的汇聚。如此一来，我们在计算梯度时，只需要存储小图上的中间结果即可。

### 采样方式

那么在图的结构上做采样是什么样的呢？文中列举了四种策略：

1. 随机选点与1近邻采样策略
2. 随机游走采样策略
3. 随机游走与1近邻结合采样策略
4. 随机点扩散采样策略

通过这些采样策略，我们每次只在子图上做计算，可以避免图的大小过度增加。当然，哪种采样策略会表现比较好，这依然取决于具体的图长成什么样子。

## 批量计算 Batching

从性能上考虑，我们不想对每一个顶点进行逐步地更新，这样会导致每一步的计算量太小，不利于并行。我们希望像别的神经网络一样，能够把这些小样本一起做成一个小批量，将其变为一个大矩阵或者一个张量 tensor 进行计算。

这里存在的问题是，每一个顶点的邻居个数可能是不一样的，那么如何将这些顶点和它们的邻居通过合并操作，并成一个规则的张量，这是一个有挑战性的问题。

## 归纳偏置 Inductive Biases

任何一个神经网络，或者任何一个机器学习的模型，都有一些假设在里面。因为如果我们不对这个世界做假设的话，那么我们实际上什么东西都学不出来。

在CNN中，这种假设是空间变换的不变性；而在RNN中，这种假设是时序的延续性。

那么，对于一个图神经网络，这种假设是置换不变性，或者说点序无关性，即前文提到的 Permutation Invariances 。也就是GNN可以保持图的“对称信息”，不管我们如何交换顶点的顺序，GNN对其的作用都是保持不变的。

## 聚合操作对比

无论是求和、取平均值或者求最大值，其实没有一种是特别理想的，因为这些操作理论上都无法区分两个网络的输入。或许我们可以有更加好的设计，来做到从聚合操作上区分网络的输入（傅里叶变换？）。

## GCN作为子图的函数近似

> GCN as Subgraph function approximators

GCN means Graph Convolutional Network，图卷积神经网络。

GCN如果有k个层，每一层都只看一个它的邻居的话，就有点类似于在卷积神经网络里面，k层3\*3的卷积。如果从这个角度上思考，每一个最后的顶点，它能够看到的其实是一个子图。

这个子图，它的大小是k，也就是最远的顶点距离当前顶点的距离是k。那么，每个点其实都是看以自己为中心的，往“外”走k步的那个子图的信息的汇聚。GCN也就可以被认为是有n个这样的子图，每个子图就是以顶点为中心向外走k步，并且在这个每个子图上求一个embedding出来。

但是这一段作者并没有讲得非常明白，可以作为一个学习的方向。

## 点、边对偶

> Edges and the Graph Dual

从图论的角度来说，我们可以将点变成边、边变成点，然后保持原图的邻接关系。作者在文中说，我们可以用同样的思路在GNN上进行处理。

## 图卷积与矩阵乘法

核心思想：在图上做卷积操作，或者在图上做随机游走 (Random Walk) 操作，等价于把图的邻接矩阵拿出来，然后做一个矩阵的乘法。

这里类似于 Page Rank 技术，它就是在一个很大的图上，做随机游走操作，基本上就是拿着邻接矩阵出来，并且跟一个向量不断地做乘法，这就是 Page Rank 的实现。

那么，图卷积和矩阵乘法的关系，也是我们如何能对整个图神经网络得到一个比较高效的实现的一个关键点子所在。

## 图注意力网络

[[Graph Attention Networks]]，需要继续学习。

之前我们在讨论图上的汇聚操作和卷积操作的关系时，多多少少有提到过这个问题。

- 汇聚：每个顶点和它邻接的顶点的权重加起来
- 卷积：每个顶点和它邻接顶点做加权和

同样的，我们也可以在图上做**加权和**。但是我们需要注意的是，卷积的权重是跟位置相关的，就是在卷积的如3\*3大小的窗口中，每个固定的位置上有固定的权重。

但是，在图神经网络的图中，我们就不需要有这个位置信息，因为每个顶点的邻居个数是不变的，而且这些邻居是可以随意打乱顺序的，所以我们需要整个的权重对于位置是**不敏感**的。

一种做法是类似于注意力机制的做法，就是权重取决于两个顶点向量之间的关系，而不再是顶点的位置在什么地方。比如，两个顶点的向量做点乘的操作，再在这个上面做一个 SoftMax。

在使用 Attention 机制之后，我们可以得到每一个顶点，都给它一个权重，此时我们再按照这个权重加起来，就能够得到Graph Attention Network。

## 图的可解释性

> Graph Explanations and Attributions

就是我们在图上训练一个神经网络之后，我该怎样去看它到底学到的是什么东西？这里提到的方法比较简单，比如，我们可以把一些**子图**的信息抓取出来，就能粗略地了解它到底学到了什么有意思的东西。

Because realistic and challenging graph problems can be generated synthetically, GNNs can serve as a rigorous and repeatable testbed for evaluating attribution techniques.

因为现实中的有挑战的图问题都可以综合地生成，所以 GNN 可以作为评估归因技术的一个严格的而且可重复的测试平台。

## 生成模型 Generative Modelling

我们的图神经网络是不改变图的结构的，但是我们也想生成图出来，这里的核心在于，怎样对图的拓扑结构进行有效的建模。文章中也提到了一些方法，例如自编码器 Auto-Encoder 架构、自回归模型 Auto-regressive Model、深度增强学习等等。

## 总结

整体来说，这一章节讲述了一些跟GNN相关的各个技术，并且稍微提了一下它们分别是什么东西，以及大概在做什么事情，但是并没有特别深入地去解释。

# 反思 Final Thoughts

## 文章流程

本文介绍了什么是图，图的属性应该用向量来表示，对于顶点、边、全局信息。然后，文中说到了现实生活中的一些数据如何表示成图，我们又该如何在图上进行预测。

接下来，文章介绍了机器学习算法用在图上的时候，会遇到什么挑战，并且开始真正意义上地讲解 GNN。

首先，作者先定义了什么是GNN，它是对属性做变换，但是不改变图的结构信息。而且作者给了一个最简的例子，由三个 MLP 构成了一个最简的GNN的层。其次，我们该如何做预测？也就是加上一个最后的输出层。

此时，如果属性有缺失该怎么办？作者介绍了一种叫聚合的操作，把“周围”的属性拿过来，补足缺失的属性来进行预测。

然后，作者开始介绍什么是真正意义上，我们实际使用的GNN。就是在每一层中，我们通过汇聚这个操作，把所有的信息传递起来，包括了每个顶点、每条边、以及全部的全局信息。

在每一个层里面，我们能够充分地把图上的信息进行汇聚之后，那么 GNN 是能够很有效地去对整个图的结构做一些发掘的。

接下来是实验的部分，作者对于每个超参数，在他的数据集上跑了很多的结果，并且给读者展示了每个超参数对于模型结果的影响是什么。

最后，作者对于GNN相关的一些问题进行了简短的展开。

## 优点与小缺陷

文章中的图非常漂亮，制作也非常精美，同时这些图还是可交互的。这些图可以容纳更多的信息，使得图既不会很脏，又能够通过与读者的互动去找到读者真正想要的东西。

但是图，既是一个优点也是一个缺点。因为这种交互图的制作非常困难，以至于如果我们想要写一篇这样的文章，则至少需要两个作者以上。一个作者需要对机器学习比较了解，另外一个作者需要对数据的可视化、Java Script 作图非常了解。能够同时懂两种的人会比较少，那么写这样的文章的门槛就会变得很高。

有时候，相比于用图，或许用公式来描述一件事，可以更加简洁而且准确。同样一件事，用图+文字的方式，有时会显得比较冗长。代码可以给出很多细节的东西，而在这里使用图或者文字，会比较困难。如果能够结合图、公式、代码、文字多个维度，让读者进行充分地了解，可能会是一种比较好的方式。

文章中宽度的展开有一点画蛇添足，因为文章之前已经讲得非常好、非常透彻了。但在后面进行展开的时候，并没有每一个都讲得非常清楚，仅仅是告诉了读者这是什么。读者读完这些内容以后并不能知道这具体是什么东西，如果不能深入到细节，读者读过就忘的话，不如不在文章中体现这些内容。

## 对 GNN 评价

图是非常强大的工具，基本上所有的数据都可以表示成一张图。但是，它的强大也带来了它的问题，就是在图上做优化是非常困难的，因为它是一个稀疏的架构。它的每一个结构都是动态的，这使得如何有效地在CPU、GPU甚至是加速器上进行计算是一件很难的事情。

另外，GNN对超参数非常敏感，整个网络架构长什么样子、我们如何对图进行采样、以及整个优化方式，GNN其实都是非常敏感的。

这两个原因都会导致GNN的门槛比较高，虽然这里的工作在过去三四年间已经吸引了非常多的研究者，但是它在工业界上的应用目前来说仍然较少。所以，在这个问题上，还需要一些年的积累。

继续学习！
