# 视频资源

[零基础多图详解图神经网络（GNN/GCN）【论文精读】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1iT4y1d7zP/?spm_id_from=333.880.my_history.page.click&vd_source=5c0fccc3e62acb7264f3cd05395b00c0)

[【GNN 图神经网络】直观透彻理解_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1nu411e7yb/?spm_id_from=333.788&vd_source=5c0fccc3e62acb7264f3cd05395b00c0)

# 博客文章

[A Gentle Introduction to Graph Neural Networks (distill.pub)](https://distill.pub/2021/gnn-intro/)

# 引入

> 关于GNN...

## 如何处理数据

1. 把数据表示成图
2. 前一层的顶点由后一层它的邻居计算而来，图的层数越深，结点就能处理很大范围的信息
3. 定义问题

## 如何定义问题

1. 对图进行分类（可以用正常的编程语言解决；对于复杂问题，可以借助GNN）
2. 顶点层面的任务，类似聚类
3. 边层面的任务，例如语义分割后，预测每一条边上的属性是什么

## 挑战

1. 如何表示**图**，使得其与神经网络是**兼容**的。
2. 矩阵可能非常巨大(n个顶点有n\*n)，就需要压缩。
3. 想要高效地计算稀疏矩阵是一个很难的问题，尤其是在GPU上计算稀疏矩阵时。
4. 图的顶点顺序交换，图会变化，但是神经网络需要保证结果不变。
5. 如何用神经网络来处理。

# 如何存储

> 要求：存储高效且顶点顺序无关(Invariant to Node-Order)

## 点 (Nodes)
每个点使用的属性使用**标量**来表示，也可以使用**向量**来表示
## 边 (Edges)
每条边也使用标量来表示，同时也可以使用向量
## 全局信息 (Global Context)
全局信息可以用标量，也可以用向量来表示
## 邻接列表 (Adjacency List)
- 该邻接列表的**长度**与**边**的数量相同
- 第i个项表示的是第i条边连接的是哪两个节点
- 存储上，只存了边和属性，所以较为**高效**
- **顺序无关性**：
	- 因为可以将边的顺序任意打乱，只需要将邻接列表的边的顺序也相应打乱就可以了；
	- 同样的，对于顶点顺序，也可以用相似的操作（更新邻接列表中对应的数字）来保证点顺序无关性（Node-Order）

# Graph Neural Networks

## 定义
GNN是一个对图上所有的属性(attributes)，包括顶点、边和全局的上下文(context)，进行的一个可以优化的变换(transformation)，这种变换可以保持住图的对称信息。
## 对称信息 (symmetries)
当我们把这些顶点进行另外一种排序之后，整个处理结果是不会变的。
## 信息传递神经网络
"message passing neural network"框架，当然GNN也可以用别的方式来表述。
## 输入/输出
秉持着"graph-in, graph-out"的理念，GNN的输入是一个图，输出也是一个图。它会对那些属性(embeddings)，也就是顶点、边、和全局的那些向量进行变换。但是它并不改变图的连接性(connectivity)，也就是哪些边邻接了哪些顶点的信息，在进入GNN后是不会被改变的。

# 一个最简的GNN

## 做法

对于所有的全局信息向量、顶点向量、边向量，我们都分别构造一个MLP(Multi-Layer Perceptron)，也就是多层感知机（相当于全连接层）。

这个MLP的输入和输出的大小都是一样的，取决于输入的向量。这三个MLP就组成了一个GNN的层(Layer)，该层的输入和输出都是一个图。

## 层的作用

对于顶点、边、全局的向量，分别找到对应的MLP，把这些向量放入MLP，然后输出，并作为它对应的更新。

1. 这些属性被更新过以后，图的结构没有发生变化，即满足了第一个要求：只对属性进行变换，但是不改变图的结构。
2. 因为MLP是对每一个向量独立作用的，而不会考虑所有的连接信息，所以，在此处不论对顶点做任何排序或其他操作，都**不会改变结果**。

至此，这个最简单的GNN层也就满足了之前提到的要求。当然，我们可以叠加这些层，以构造一个比较深一些的GNN。

# 预测 (Prediction)

> 最后一层的输出如何得到我们需要的预测值呢？

## 对顶点做预测

与一般的神经网络没有太多区别，因为顶点已经有了一个向量的表示。只要在向量后增加一个输出维度（预测值）的全连接层，再加一个Softmax函数就可以得到我们需要的输出了。

若给出的是最后一层的输出（一个图），然后对于每一个顶点，进入全连接层，再得到输出，如此得到对于顶点的分类。

**注意：**
跟之前一样，不论有多少个顶点，此处都只有一个全连接层。也就是说，所有的顶点都会共享一个全连接层里面的参数。在之前的GNN层中，不论图有多大，都是三个MLP，所有的顶点、边、全局信息都共享一个MLP。因为这里提到的是**最简单的**情形。

## 略微复杂的情形

假设：我们仍然想要对顶点做预测，但是没有顶点向量，该如何处理？
- 此时，我们会使用到一个技术叫作**Pooling**，称为**汇聚**。
- 在CNN中，我们其实已经见过它了，与此处的Pooling没有什么本质上的区别。

我们可以将与这个点**相连的边**的向量都拿出来，同样把**全局**的向量拿出来，然后把这些向量全部**加起来**，就会得到代表这个顶点的向量。有了这个向量，我们就可以通过最后的全连接层来得到输出。
此处假设顶点、边、全局的向量维度都是一样的，如果不一样，则需要做一些投影。

## 对边进行预测

假设我们没有边的向量信息，但是有顶点的向量，此时想要对边进行预测。那么同样道理，我们可以把顶点的向量**汇聚**到边上。

一条边连接两个顶点，然后这两个顶点向量我们可以把它们加起来，或者再加上全局的那个向量，来得到这个边的向量。然后进入边向量的输出层，也就是所有的边共享一个输出层，最后得到这个边的输出。

## 对整个图做预测

假设我们没有全局的向量，但是有顶点的向量，目标是对整个图做预测。

我们可以将所有顶点向量加起来，得到一个全局的向量，然后进入全局的输出层，并得到一个全局的输出。

总的来说，不论我们缺乏哪一类的属性，我们都可以通过汇聚（Pooling）这个操作，得到我们需要的那个属性的向量，最终产生我们的预测值。

## 总结

最简单的GNN如何构成？

给出一个输入的图，首先进入一系列的GNN层，每个层中有三个MLP（多层感知机），分别对应三种不同的属性（点、边、全局）。

逐一经过这三个MLP后，会得到最终的输出，其保持了整个图的结构，但是里面所有的属性已经发生了变化。最后根据我要对哪一个属性进行预测，再添加合适的一些输出层，如果我们是缺失信息的话，我们就加入合适的汇聚层，就可以完成我们要的预测了。

## 局限性

虽然这个情况很简单，但是我们可以发现其有很大的局限性。

主要的问题在于经过GNN层时，我们**并没有**使用图的**结构信息**，只是每个属性进入自己的MLP，并没有利用比如这些顶点是与哪些边相连的，或者与哪些顶点是相连的；一条边是连接了哪些顶点跟另外哪些边是相连的，这些连接信息都**没有**考虑到。

这导致了最终的结果可能并不能够特别充分利用(leverage)整个图的信息。

# 信息传递 (Passing Messages)

为了改进上述的局限性，我们使用到了名叫**信息传递**的技术，与之前的汇聚的思想上有些许相似。

我们可以将与这个点相邻的那些点，也加入到这个**汇聚**中来，让信息可以尽早地融入我们的其他向量中。
