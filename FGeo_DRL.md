# Abstract
FGeoDRL是一个神经-符号系统，神经部分是利用形式化强化学习环境构建Distil-Bert训练的策略网络，然后利用蒙特卡洛方法进行启发式搜索。符号部分是利用FormalGeo构建的强化学习环境，这个环境将FormalGeo中的GPS（Geometric Problem Solver）方法建模为强化学习中的MDP过程

与First Step中的解题过程被设定成超树相同，题目的已知条件和目标被设置成状态空间，定理集被设置成动作空间
# Introduction
llm强大的表示学习能力，能够使得机器理解和学习人类的语言，使得llm可以作为工具来指导用户对平面几何题目进行分析和建议

受限于自然语言的影响，通过llm无法直接为平面几何题目的解答提供精确答案，llm的建议与分析只能作为灵感来源与提示，并不能直接作为解答。在处理IMO等级别的题目的时候，我们需要精确的解释，并且可追溯可验证，这样的需求是llm模型无法提供的。
#### 局限性
llm模型处理平面几何问题具有以下的**局限性**：
1. 由于神经网络的不透明性，我们难以理解模型是如何处理输入问题的(**形式化的重要性**)
2. 由于缺乏精确的评估指标，我们缺乏一套精确的方法或标准来评估和验证使用定理序列解决几何问题的过程是否完整和正确。换句话说，当我们利用定理来推导和解决问题时，我们没有有效的工具来检查这个解决方案是否真正地、完全地解决了提出的问题。(**GPS建模MDP过程的重要性**)
3. 由于逻辑自然语言处理需要基于人类理解逐步推导，因此仅仅使用语言模型来推理并直接确定几何问题是否可以被解决，是一项挑战。因为语言模型主要处理的是文本数据，它们在处理严格的逻辑推理和精确的几何问题时可能不够精确或透彻，无法像人类那样完全理解和推演问题的每一个细节。(**建立以强化学习为手段的推理过程**)
#### 解决方案
为了解决上述的局限性问题：我们建立来FormalGeo形式化系统，将文本与图片统一转换为确定的形式化语言以确保计算机可以准确识别与应用；其次，我们建立了名为Geometry7k的数据集，结合了当前许多数据集的优点

#### Why FGeo-DRL
为什么我们需要FGeo—DRL？因为训练llm需要大量的数据，这是我们的数据集做不到的(数据不足)。因此，我们使用FGeo—DRL，该系统利用了神经符号框架，这种框架结合了神经网络的学习能力和符号逻辑的严格性，能够自动学习和推导解决几何问题的策略，而不需要人工干预。这一点使得它在处理几何问题时的效率和效果都远超传统方法。
该系统还采用了两种先进的技术：深度强化学习和蒙特卡洛树搜索。深度强化学习使得系统能够通过试错来优化其决策过程，而蒙特卡洛树搜索则提供了一种高效的方式来探索和评估可能的解决方案路径。蒙特卡洛树搜索是深度强化学习的实现方式

#### 延迟奖励
我们采用延迟奖励来处理搜索期间生成的奖励反馈。蒙特卡洛树搜索的使用使我们能够获得一个搜索轨迹链，并根据**问题最终是否解决**来反馈奖励值。奖励的反馈是基于最终结果，即问题是否被解决，而不是立即给出。这种方式有助于减少奖励设定的主观性和复杂性。
$DRL \Rightarrow MCTS\Rightarrow延迟奖励$

#### 具体构成
预训练模型在神经网络部分发挥作用(初始化/分类任务)，深度强化学习和MCTS在符号部分发挥作用(不断改进预训练精度/准确性)
#### 三大贡献
1. 构建数据集和FormalGeo形式化系统，解决局限性1
2. 构建FGPS几何问题求解器，使得给予形式化系统下的问题解决的评估指标得到确认，解决局限性2
3. 引入深度强化学习和MCTS，提升启发式搜索的效率，完善推理过程，提升推理能力，解决局限性3
# Related Work
#### 早期形式化工作
这些工作都建立了自己的形式化系统，并且还有基于符号推理的解题器
InterGPS：使用Geometry3k作为数据集,一共有3002个问题
GeoS：一共有186个问题

从数字上我们可以看出：这样的数据量太小，他是一个活字典而非真正的专家
#### 当前的llm解题
GeoQA与UniGeo将解题过程视为多模态问题而非符号系统的推理，这样带来的问题显然是连接主义的通病：这样的回答是不可读 不可验证 无法回溯的

#### 我们的贡献
我们统一Geometry3k、GeoQA 和 GeoQA+等数据集，做成了我们的Geometry7k，将所有的题目全部形式化FormalGeo可读的文件，用于后续推理
这个数据集提高数据的一致性和可比性，为我们的研究提供了更强大和更全面的基础
数据集为我们后续做推理提供了基础和分析解题模式的可能性
#### 强化学习与蒙特卡洛搜索
近年来的各种研究都表明：强化学习在推理领域大有可为
目前，有研究者利用MCTS完成数学定理证明[18]，有研究者利用深度强化学习完成了有关交互式定理的推理的研究[19]，还有研究者利用DRL进行数学单词预测[20,21]，聚焦到细分的平面几何定理领域，有人利用深度强化学习来对平面几何定理的推理进行了一定的推动[22]，在我们的研究中，我们使用DRL中的MCTS方法。由于即时奖励函数不够好，我们使用延迟奖励而不是即时奖励，这样的定义方式勒事AlphaGo，并且可以缩短定理序列的长度，实现最优定理序列
# Geometry Formal System
形式化系统对于使用人工智能解决平面几何问题是极端重要的
我们创建的数据集基于我们自己的形式化系统，这二者之间的一致性与统一性可以保证没有其他干扰因素对推理产生印象，运用我们的体系可以使得研究专注于推理本身而不用担忧干扰因素的影响
**数据集与形式化的重要性**
为了验证我们的推理结果是否正确，我们在基于FormalGeo的形式化系统下开发了几何问题求解器FGPS
**解决评估指标的问题**

#### 形式化语言
形式化语言的两个工具分别是**GDL(几何定义语言)**与**CDL(条件声明语言)** ，这其中，**GDL是CDL的基础，CDL是通过使用GDL中的词汇来实现的**。

GDL包括谓词定义语言（Shape, Collinear, Cocircular，Equal，Parallel，Perpendicular   ）和定理定义语言(theorem)，一共有88个谓词
谓词定义语言用于定义不同类型的几何关系和几何属性。典型的谓词定义语句包括几何关系、点变量、实体存在约束、格式有效性约束和自动扩展的名称。求解器可以阅读和解释谓词定义语言，以确保输入问题条件的合法性。谓词定义语言可以分为结构谓词和自定义谓词

定理定义语言一共有196条，作为解题的条件覆盖当前题目所需要的所有条件，并且在后续的研究中，随着题目难度的变化，定理定义语言的库存可以增加或者减少，体现了灵活性。
定理语言由前提和结论组成，几何关系和属性作为前提，并在定理的判断规则下得出结论。

CDL：**声明各种各样的条件**：构图条件，题设条件，目标条件
构造语句描述了几何问题图的TSI，如基本形状、共线和共圆。条件语句用于输入几何问题中已知条件，包括几何和代数关系。目标陈述声明声明解决问题的目标。
#### Geometry7K数据集

![[Pasted image 20240428200529.png]]
如图所示，我们展示了如何通过我们的形式化系统完整的将一道由图片和文本构成的题目完全以我们的形式化系统得以展示。
除了将问题形式化后，我们还将对应题目所需要的定理序列人为标注，为后续的推理工作作为参考，这些定理已经通过FGPS的检测

同时，我们还根据**定理的长度**为题目的难度做划分
![[Pasted image 20240428201332.png]]
$L_1=1-2,L_2=3-4,L_3=5-6,L_4=7-8,L_5=9-10,L_6>10$

# Reasoner(推理器)
在使用FormalGeo形式化系统做推理工作时，应用定理的过程被抽象成生成超树的过程。

#### 超树
超树的构成是节点(node)和边(edge)。每个node是由题目的初始化条件(根节点)或者应用定理进行推理之后得到的条件构成。每个edge都是一个边
当多个节点通过定理连接起来后，会生成新的条件
每当新的条件生成，我们会将这个条件与目标条件进行匹配，如果匹配，那么可以视作问题解决

#### 结合强化学习的推理器
由于我们通过预训练模型，从数据集中学习到了一定的先验知识，所以我们可以用这些知识作为指导。因此，我们可以将题目所给出的条件和图像信息作为起始状态，然后利用先验的知识从初始状态开始选择定理，推动题目解决。

#### 状态空间&动作空间&目标
状态空间由当前的条件和**所有可扩展信息**组成，被定义为$S_t$
动作空间由基于当前状态下所有可以执行的定理组成，被定义为$a_t$
题目所要求解的目标被定义为$g$

#### 奖励函数
本研究中使用的奖励函数为delayed reward(延迟奖励)。这是因为我们的目标是达到successful，即解题成功，因此我们只需要关注最终结果，而不是关注每一步的选择(及即时奖励)

**具体来说**：当一个问题可以通过我们的推理器得到解释，那么推理器会为解决这个问题的定理序列赋予一个奖励函数，大小为1；当一个问题因为种种原因：预测时间过程、定理序列长度过长或者基于当前状态下无定理可以执行，那么我们会赋予这个问题的定理序列奖励函数，大小为0

**或许有人会有疑问**，当题目不可解的时候为什么奖励函数是0而不是-1？**这是因为**：失败的奖励值设置为 -1，则倾向于排除在后面的探索中应该选择的定理。因此，当奖励值设置为 0 时，它更显着地将学习集中在成功解决问题的序列上。

#### 动作空间 action space
如上所述，动作空间由196条定理构成，如有需要，我们可以通过添加定理的形式来扩展动作空间,通过基于当前条件下应用定理的形式，我们可以得到更多的条件，来扩充状态空间。那么基于当前的定理数量，是否所有的节点都可以应用这196条定理呢？**并不是**

这种情况的出现主要是由于构建图的严格要求，需要考虑各种场景。例如，在确定两个三角形的一致性时，如果已知两组角度相等，那么只要一组对应的边相等，就可以建立证明。每条边在图构造中都有一个严格的定义;因此，这个定理需要扩展为三个分支，形成三个新的定理，每个定理对应不同的边。因此，对于定理库中的 196 个定理，如果定理和图之间没有歧义，则不需要扩展，并且假设默认只有一个分支。如果存在歧义，则执行分支扩展。最终，我们可以得到 234 个可执行定理，这意味着**每个状态节点有 234 个可能的动作**。

我们用一个例子来说明
![[Pasted image 20240430162221.png]]
#### 由预训练模型带来的指导——guidance
对于平面几何数学问题，任何特定状态都有 234 个可执行动作。因此，如果解决几何问题需要很长的定理序列，需要探索的搜索空间将呈指数增长。此外，更复杂的问题通常需要构建复杂的图，从而增加将定理应用于这些详细图所需的时间。这种在时间上的扩展显然不适合搜索过程。在随机搜索的背景下，需要广泛的探索来仔细估计问题的真实概率分布。如果探索阶段消耗的时间太多，可能会导致对实验产生不可预测的影响。因此，需要应用经验知识来指导定理应用。为了解决这个问题，我们使用了一个轻量化的Transformer模型DistilBert，我们使用DistilBert来处理了Geometry7K数据集。通过在环境中应用带注释的定理序列，我们获得了每个节点的状态。状态连同执行定理一起保存到经验池中作为 $Exp(s_t, a_t, G_t)$。然后，我们利用策略网络来学习和预测当前状态的定理选择。

#### 蒙特卡洛树搜索——MCTS
由于数据量明显低于语言模型训练所需的数据量，仅依靠策略网络进行定理应用是不够的。我们需要在**搜索过程**中使用策略网络，其中它的包含大大减少了搜索空间。

在我们的研究中，为了使得MCTS能够应用，我们重新定义了Node和Edge:
节点$S$被描述为由当前状态下的所有条件信息组成，而边$Edge$被描述为动作空间中的可执行定理。节点信息包括每个节点-动作对的访问次数 $N(s,a)$、先验概率$P(s,a)$ 和通过在该节点上执行相应的动作获得的累积奖励$G_t$
#### 蒙特卡洛树搜索的四个过程:选择 扩展 模拟 回溯
**Selection：** 从根节点(也就是题目的条件开始的状态)选择定理的方式，并不是完全依照先验概率$P(s,a)$，而是要一定程度上参考另一部分取决于探索期间选择的实际探索次数。这主要是为了使探索更加全面，避免了连续执行具有高先验概率的动作的缺点。具体的设计就是下面的公式。
![[Pasted image 20240430193607.png]]
$G_t$代表在根节点状态下，由先验概率$P(s,a)$引导探索所获的平均奖励，这个对数项涉及在状态$s_t$下选择动作$a_i$的次数与当前考虑的动作$a_t$被选择的次数的比例。从$i=0$到$T$的总和可能是结合历史数据或过去的经验，直到某个时间范围$T$。
**Expansion：** 经过Selection选择好动作后，自然的，我们会扩展动作执行产生的状态节点。这种扩展包括使用策略网络来预测可选择定理并利用先验概率来提供选择偏好。随后，在树中添加了未扩展的动作，通过执行$a_t$的动作来创建一个新节点$s_{t+1}$。这样做的目标是通过模拟的结果，为新扩展的节点初始化统计信息来进一步探索所选节点。
**Simulation：** 在模拟阶段，我们会模拟扩展的状态节点。我们在策略网络的指导下设置了最大搜索步骤和模拟计数，以在较小的搜索空间中获得更可靠的模拟结果。对于模拟结果，如果目标节点在达到最大搜索步骤前得到结果，则返回 1 的奖励；否则，返回 0 的奖励。通过多个模拟过程，我们得到了扩展节点的平均分数$G_t$。
**Backup：** 对于模拟阶段获得的奖励分数和访问计数，我们需要进行反向传播。通过反向传播，我们将仿真结果$W(s_t,a_t)$反馈给搜索树来更新累积奖励$G_t$，从而影响未来的定理选择。
![[Pasted image 20240430193550.png]]


通过蒙特卡洛树搜索，探索阶段收集的数据可用于改进策略网络，主要目标是使用从实际探索中获得的累积奖励来提升策略网络在实际预测中的表现。

最开始的时候，强化学习网络和通过distilbert训练的策略网络参数一致，并且强化学习网络在训练阶段使用的是策略梯度进行优化(直接对策略进行优化
#### MCTS提升策略网络的过程
使用蒙特卡洛树搜索的时候，我们会得到一个累积反馈奖励$R(\tau)$
![[Pasted image 20240430191236.png]]
上面的展示了蒙特卡洛树搜索如何执行轨迹预测。我们的目标是找到最大化目标函数$R(\tau)$的参数 $\theta$，即找到一个最大化 $E(R(\tau))$的策略。其中$\tau$表示从初始状态到终端状态的轨迹，$R(\tau)$ 是沿该轨迹获得的累积奖励，$E(R(\tau))$表示收集到的多个轨迹的预期累积奖励。
![[Pasted image 20240430191637.png]]
其中$\pi_{\theta}$表示定理选择策略。为了防止策略网络的过度拟合，我们利用监督学习获得的策略网络作为模拟阶段的定理预测器。该预测器的参数与之前的强化学习策略网络生成保持一致。
**在定理选择和扩展阶段，我们采用最新的经过MCTS提升表现后的策略进行定理选择。**

为了最大化累积奖励，我们需要优化路径策略，使其能够选择具有最大累积奖励的策略。我们使用策略梯度来更新策略参数：
![[Pasted image 20240430191800.png]]

#### 网络架构
![[Pasted image 20240430191951.png]]




# Experiments
我们将数据集中的每道题目全部划分为状态-动作对$(s_t,a_t)$，并通过强化学习环境深入研究带注释的定理。通过参考基本事实答案，我们可以捕获在定理执行的每一步获得的状态$s_t$和应用的具体定理。

我们将具有6981道具有ground truth的Geometry7K数据集以 0.7:0.15:0.15:0.15 的比例将它们分为训练集、测试集和验证集，每个部分得到的$(s_t,a_t)$对分别是20,981, 4470, and 4470 

下表是我们的策略网络对于预测的初步实验。这里的“Range”指的是可选择定理的数量，而“Hit rate”表示定理由基于当前状态的策略网络预测，按概率从高到低排序，并选择n范围内的许多定理。数据显示，随着范围从 1 增加至 25，命中率有显著提高，从而证明了策略网络在缩小搜索空间方面的有效性。如果在确定范围内的定理与现实中手动注释的定理相匹配，则该定理被认为是命中可解定理序列
![[Pasted image 20240503083551.png]]

#### baseline基准模型
首先我们要介绍在FormalGeo系统下的一些搜索策略与方法
#### 搜索方法:
forward search(FW):将应用定理后得到的状态转化为条件，直到条件和目标对齐
backward search(BW):将目标扩展成子目标，直到子目标和条件对齐
#### 搜索策略：
BFS DFS RS(Random Search) BS(Beam Search)

本研究中，在FormformGeo7k 数据集上，通过使用策略网络和蒙特卡洛树搜索进行强化学习，我们设置了 30 次模拟，每次最大模拟长度为 30 步。尽管数据集中大多数定理序列不超过 10 步，但模拟步长设置过长会导致模拟时间显著增加，影响效率。因此，对于超过 10 步的定理序列，需要特别设定搜索参数。表 3 中粗体文本展示了不同难度问题的最佳结果。
FGeo-DRL在解决所有问题中都有提升，在解决难度L4到L6的题目时候，提升更大
![[Pasted image 20240503090332.png]]
**PN：Policy Network**
#### Ablation Study 消融实验:理解模型的工作机制和进一步改进提供了直观的依据
在FormalGeo7K数据集上进行了三种情景下的消融实验。
首先，通过策略网络直接预测定理序列，每个实例应用的定理数量分别设为1，3，5，目的是探索策略网络的直接准确性。
其次，使用模拟搜索展示策略网络对状态的泛化能力，在搜索树的每个节点执行最可能的n个定理作为下一个节点的初始状态。此外，结合常规搜索与策略网络剪枝的效率，通过波束搜索模拟策略网络剪枝的影响。通过模拟搜索的方式引入混淆因素增强泛化性能
最后，评估蒙特卡洛树搜索与策略网络结合在FGeo-DRL中带来的改进。


图3中，左图的横轴表示问题的难度级别，右图的横轴表示问题类型。实验结果表明，蒙特卡洛树搜索的辅助显著提高了从L1到L3级别的问题解决成功率，这很大程度上依赖于FGeo—DRL通过采样反馈的方式改进了策略网络；对于解决序列较短的问题，成功率更高，因为在有限的步数内更可能达到解决方案的终点；在到达终点时，奖励反馈有助于策略网络学习解决问题的策略。特别是在数据集中涉及面积计算的问题，尽管数量较少，但由于解决序列较短，解决率明显高于解决序列较长的问题，如周长计算。对于解决序列较长的问题，性能更多地依赖于策略网络。对于问题数量较多的角度和长度问题，它们的数值优势允许从短序列到长序列的有效过渡，在这两个类别中取得了显著的改进。
![[Pasted image 20240503105427.png]]
![[Pasted image 20240503110230.png]]

表4汇总了消融实验的结果。从表中可以看出，直接应用PN解决问题就能达到40多的解决率，随着每次尝试中应用的定理数量增加，使用策略网络的总体准确率提高。结合波束搜索的方法（PN + BS）在同等定理数量下表现更好。FGeo-DRL方法在不指定定理数量的情况下，展现出最高的成功率（86.40%）。蒙特卡罗树搜索和策略梯度学习的加入引入了模拟搜索和改进重新调整的机会，大大支持了 FGeo-DRL 在解决复杂问题方面的演绎推理优势和搜索效果。
![[Pasted image 20240503105435.png]]
#### Discussion
传统方法在处理稀疏数据和有限探索能力方面存在局限，而FGeo-DRL通过采用蒙特卡洛树搜索等强化学习技术，有效应对复杂几何问题。这一观点与Silver等人在AlphaGo开发中的发现一致，即蒙特卡洛树搜索对提升学习效率和决策深度至关重要。虽然随机抽样定理能减少手动注释时间，但可能不适合解决操作序列长的问题。手动注释虽增加标签压力，但有助于推理引擎快速识别并解决类似问题。
#### FGeo_DRL相比较传统推理和人类的优势
1. 相比较传统的推理系统，FGeo-DRL 在几何问题推理中表现出良好的可解释性。这可以归因于形式系统提供了广泛的定理集合以及通过定理推理生成的推理树这一事实。FGeo-DRL 通过记录在每个状态下选择的定理，形成一个定理序列，从初始节点扩展到终端节点。
2. 此外，FGeo-DRL 可以生成与手动注释序列不同的问题解决序列。在演绎推理过程期间，该模型为状态节点处的 234 个定理活动中的每一个提供了明确的概率分数，并在蒙特卡罗树搜索过程期间探索了高概率解决方案。
3. 在某些情况下，FGeo-DRL 甚至可以识别更简洁的路径来解决问题。例如，如下图所示，FGeo-DRL 倾向于使用余弦定理而不是 Pythagorean 定理。这种方法不同于手动注释中使用的方法，并导致更精简的定理探索路径。
![[Pasted image 20240430194714.png]]
# Conclusion
本文提出了一种基于强化学习框架的定理序列预测模型FGeo-DRL。我们实现了一个预训练的自然语言模型进行搜索修剪。此外，我们将蒙特卡洛树搜索与通过符号推理系统构建的强化学习环境进行交互。通过对我们提出的几何问题数据集 FormformGeo7k 进行搜索，我们验证了几何问题的演绎推理。FGeo-DRL 表现出卓越的可解释性，利用引导搜索和交互反馈来纠正经验网络。它在探索几何问题的最短和新颖的解决方案方面发挥着积极的作用

# 未来可以解决的方向
然而，尽管 FGeo-DRL 在 FormformGeo7k 数据集上表现出强大的性能，但它遇到了某些限制。  
 1. 在解决难度高的问题时，它需要越来越多的模拟和搜索步骤来产生令人满意的结果，无意中延长了搜索持续时间。
 2. 依赖于手动注释的数据，包括图形细节和问题描述，它们在整个几何问题解决过程中提供了必要的见解和指导。
 **在未来的工作中，我们计划解决这个多模态挑战。**
# 要问的问题
这里的意思是不是说，只依靠策略网络，按部就班是不行的（反之，如果数据量足够大，我们就可以只通过策略网络就行啦？）/需要讲一下具体如何用MCTS改进预训练的策略网络
Answer：

先验概率P(s,a)是不是通过策略网络得来的
Answer：mcts过程中，模拟用的是未更新先验概率，选择用的是更新过后的先验概率，这样的网络会一定程度上避免过拟合
初始状态at的选择为什么公式是长这样的
Answer：这个公式的原始形式是经典ucb形式，然后适配我们的系统
Gt这个公式是干嘛的
Answer：$w(s_t,a_t)$是经过n次模拟之后的奖励的均值，若每次模拟效果好，那么最终的均值肯定大，通过这样累计折扣的叠加奖励，我们得以最终获得较好的策略网络

MCTS所有的工作都是训练过程吧？（优化策略网络的过程）不是实际的应用过程吧
Answer：并不完全是这样，MCTS在探索阶段可以起到改进策略网络的过程(**采样反馈过程**)，比如说在策略网络中对应题目a，我们得到的定理序列是1234，但是在MCTS探索阶段，我们发现了更好的序列125，那么我们就会对应的改进策略网络，使得在正式推理的时候，我们的guidance会更加倾向于序列125来解题（更大概率）。当然，MCTS也可以用于正式的推理过程。

promt与预训练的关系
Answer：Prompt的作用
1. **指导预训练模型的行为**：通过改变prompt，我们可以影响模型的输出，让其在特定任务上表现更好。例如，在生成文本的任务中，prompt可以是一个问题或一个话题，模型则生成与之相关的回答或内容。
2. **零样本或少样本学习**：在这种情况下，prompt成为启用模型在几乎没有额外训练数据的情况下解决特定任务的关键。通过精心设计的prompt，模型可以利用其预训练阶段学到的知识来执行新任务。
3. **任务特定的调整**：有时，即使模型已经预训练完成，我们也需要通过所谓的“prompt engineering”（提示工程）来微调模型，使其更好地适应特定的应用场景。
Prompt与预训练模型的关系体现在，prompt为模型提供了一个交互界面或者执行指令，使得预训练模型能够在不同的上下文和需求下调整其行为。这种方式极大地提升了模型的灵活性和适用性，因为它允许相同的模型在多种任务上表现出色，只需通过更改输入的提示即可。

总的来说，prompt是连接用户需求与预训练模型能力的桥梁，它让模型能够在没有显著重新训练的情况下适应各种任务和环境

代码要看的部分
MCTS在哪里:MCTS文件夹agent和MCTS
/Users/leeyoung/PycharmProjects/FGeoDRL/MCTS/agent.py
/Users/leeyoung/PycharmProjects/FGeoDRL/MCTS/mcts.py
预训练模型在哪里:/Users/leeyoung/PycharmProjects/FGeoDRL/Network/SL/train_sl_network.py
数据集的$(s_t,a_t)$划分在
/Users/leeyoung/PycharmProjects/FGeoDRL/Datasets/get_train_dataset.py
/Users/leeyoung/PycharmProjects/FGeoDRL/Datasets/get_test_dataset.py
所有的json文件
进行定理预测的启动如何进行：/Users/leeyoung/PycharmProjects/FGeoDRL/Network/SL/train_sl_network.py
